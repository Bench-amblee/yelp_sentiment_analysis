{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Yelp Reviews - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepared By: Ben Chamblee - https://github.com/Bench-amblee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#Introduction)          \n",
    "\n",
    "\n",
    "* [Part 1: Machine Learning with Extracted Features](#Part1)\n",
    "    * [Logistic Regression](#Logistic_Regression)\n",
    "    * [Random Forest](#Random_Forest)\n",
    "    * [Support Vector Classification](#SVC)\n",
    "    * [Gradient Boosted Classification](#GBC)\n",
    "    * [Results](#Results1)   \n",
    "    \n",
    "\n",
    "* [Part 2: Machine Learning with Bag of Words](#Part2)\n",
    "    * [Naive Bayes with CountVectorizer](#NB_CV)\n",
    "    * [Naive Bayes with TFIDF](#NB_TF)\n",
    "    * [Gradient Boosted Classification with Count Vectorizer](#GBC_CV)\n",
    "    * [Gradient Boosted Classification with TFIDF](#GBC_TF)\n",
    "    * [Results](#Results2)      \n",
    "    \n",
    "\n",
    "* [Part 3: Embedding Techniques](#Part3)\n",
    "    * [Dense + Sparse Features](#Dense)\n",
    "    * [Naive Bayes Probability with Dense + Sparse Features](#NB_Sparse)\n",
    "    * [Stacked Model](#Stacked)\n",
    "    * [Results](#Results3)       \n",
    "    \n",
    "    \n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've collected the data, analyzed it, found the most important feautres, and have a baseline accuracy around 70%. Now we'll be going through some other methods of modeling to find the most accurate version so we can deploy it for future yelp review classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will:\n",
    "- Test out multiple different models\n",
    "- Including Regular ML Models with our extracted features and embedded models with sparse/dense text matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "yelp_data = pd.read_csv('yelp_data_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Positive_Words_P</th>\n",
       "      <th>chocol</th>\n",
       "      <th>cup</th>\n",
       "      <th>amaz</th>\n",
       "      <th>eat</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>linguin</th>\n",
       "      <th>linguini</th>\n",
       "      <th>jimmi</th>\n",
       "      <th>player</th>\n",
       "      <th>juliana</th>\n",
       "      <th>pool tabl</th>\n",
       "      <th>castl</th>\n",
       "      <th>falafel</th>\n",
       "      <th>paella</th>\n",
       "      <th>white castl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'The chocolate cups are amazing! Have been eat...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Morris Park Bake Shop has become my go to spo...</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'I thought the cookies and biscotti were prett...</td>\n",
       "      <td>0.314583</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Guys.... so Im a big time biscotti connoisseu...</td>\n",
       "      <td>0.238068</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'I had a craving for a special type of cake wi...</td>\n",
       "      <td>0.314643</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1761 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                             Review  \\\n",
       "0  Morris Park Bake Shop  'The chocolate cups are amazing! Have been eat...   \n",
       "1  Morris Park Bake Shop  'Morris Park Bake Shop has become my go to spo...   \n",
       "2  Morris Park Bake Shop  'I thought the cookies and biscotti were prett...   \n",
       "3  Morris Park Bake Shop  'Guys.... so Im a big time biscotti connoisseu...   \n",
       "4  Morris Park Bake Shop  'I had a craving for a special type of cake wi...   \n",
       "\n",
       "   Polarity          Sentiment  Positive_Words_P  chocol  cup  amaz  eat  \\\n",
       "0  0.500000           Positive          0.222222     0.0  0.0   0.0  0.0   \n",
       "1  0.338889  Slightly Positive          0.206897     0.0  0.0   0.0  0.0   \n",
       "2  0.314583  Slightly Positive          0.130435     0.0  0.0   0.0  0.0   \n",
       "3  0.238068  Slightly Positive          0.127660     0.0  0.0   0.0  0.0   \n",
       "4  0.314643  Slightly Positive          0.218750     0.0  0.0   0.0  0.0   \n",
       "\n",
       "       year  ...  linguin  linguini  jimmi  player  juliana  pool tabl  \\\n",
       "0  0.000000  ...      0.0       0.0    0.0     0.0      0.0        0.0   \n",
       "1  0.000000  ...      0.0       0.0    0.0     0.0      0.0        0.0   \n",
       "2  0.160339  ...      0.0       0.0    0.0     0.0      0.0        0.0   \n",
       "3  0.000000  ...      0.0       0.0    0.0     0.0      0.0        0.0   \n",
       "4  0.000000  ...      0.0       0.0    0.0     0.0      0.0        0.0   \n",
       "\n",
       "      castl  falafel  paella  white castl  \n",
       "0  0.000000      0.0     0.0          0.0  \n",
       "1  0.000000      0.0     0.0          0.0  \n",
       "2  0.000000      0.0     0.0          0.0  \n",
       "3  0.149255      0.0     0.0          0.0  \n",
       "4  0.000000      0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 1761 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Machine Learning with Extracted Features<a id='Part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different models we can try but to start we're going to work with the standard machine learning models like logistic regression and random forest\n",
    "\n",
    "I'll be collecting accuracy and f1 scores along the way to see which model performs the best with this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the process will look like:\n",
    "\n",
    "- Choose a model type\n",
    "- Define a pipeline and use GridSearch to find the best parameters\n",
    "- Save the model so we don't have the run the cell multiple times\n",
    "- Load the model and run accuracy and f1 scores\n",
    "- Move on to the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from pycm import *\n",
    "\n",
    "X = yelp_data.iloc[0:,4:]\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index\n",
    "\n",
    "X_train, X_test, y_train, y_test, itrain, itest = train_test_split(X,y,indices,train_size=0.8,random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression<a id='Logistic_Regression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we're going to create a pipeline that uses the standard scaler and a classifier using GridSearch to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), ('lr', LogisticRegression(solver = 'lbfgs'))] \n",
    "pipeline = Pipeline(steps)\n",
    "parameters = {'lr__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lr.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lr.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6951553930530164\n",
      "F1 Score (macro):  0.6933462169665948\n",
      "F1 Score (micro):  0.6951553930530164\n",
      "F1 Score (weighted):  0.6957884475947633\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc = test_accuracy\n",
    "lr_f1 = f1_accuracy\n",
    "lr_f1m = f1_accuracym\n",
    "lr_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest<a id='Random_Forest'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing here, first we'll make a pipeline and then use gridsearch to find the best parameters to tune this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_features': 'sqrt', 'rf__n_estimators': 50}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), ('rf', RandomForestClassifier())] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'rf__n_estimators':[10 , 20, 30, 40, 50], 'rf__max_features':['auto','sqrt']}\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rf.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rf.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6604204753199269\n",
      "F1 Score (macro):  0.6617321163064211\n",
      "F1 Score (micro):  0.6604204753199269\n",
      "F1 Score (weighted):  0.6601505940738249\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc = test_accuracy\n",
    "rf_f1 = f1_accuracy\n",
    "rf_f1m = f1_accuracym\n",
    "rf_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classification (SVC)<a id='SVC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SVC model I will unfortunately have to set the kernel to 'linear' and the gamma to 'auto' just because my laptop doesn't have sufficient computing power to train the model in a reasonable amount of time. This may not give us a perfect result, but we will still see the benefits of an SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 0.01}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "steps = [('scaler', StandardScaler()), ('svc', SVC(probability=False,kernel='linear',gamma='auto'))] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'svc__C':[0.01, 0.1, 1]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 3, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svc.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svc.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.7010968921389397\n",
      "F1 Score (macro):  0.7017782289569849\n",
      "F1 Score (micro):  0.7010968921389397\n",
      "F1 Score (weighted):  0.701150485468546\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_acc = test_accuracy\n",
    "svc_f1 = f1_accuracy\n",
    "svc_f1m = f1_accuracym\n",
    "svc_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier<a id='GBC'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.15, 'gbc__n_estimators': 500}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('gbc',\n",
       "                 GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                            criterion='friedman_mse', init=None,\n",
       "                                            learning_rate=0.15, loss='deviance',\n",
       "                                            max_depth=3, max_features='sqrt',\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=500,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(learning_rate = 0.15, max_features = 'sqrt', n_estimators = 500))] \n",
    "clf = Pipeline(steps) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.706581352833638\n",
      "F1 Score (macro):  0.7078660118314662\n",
      "F1 Score (micro):  0.706581352833638\n",
      "F1 Score (weighted):  0.7070986252081772\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_acc = test_accuracy\n",
    "gbc_f1 = f1_accuracy\n",
    "gbc_f1m = f1_accuracym\n",
    "gbc_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results<a id='Results1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our scores, let's see which model performed the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBC</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0  Logistic Regression     0.695     0.693     0.695        0.696\n",
       "1        Random Forest     0.660     0.662     0.660        0.660\n",
       "2                  SVC     0.701     0.702     0.701        0.701\n",
       "3                  GBC     0.707     0.708     0.707        0.707"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = pd.DataFrame({'Model':['Logistic Regression', 'Random Forest', 'SVC', 'GBC'],\n",
    "             'Accuracy':[lr_acc, rf_acc, svc_acc, gbc_acc],\n",
    "             'F1_Macro':[lr_f1, rf_f1, svc_f1, gbc_f1],\n",
    "             'F1_Micro':[lr_f1m, rf_f1m, svc_f1m, gbc_f1m],\n",
    "             'F1_Weighted':[lr_f1w, rf_f1w, svc_f1w, gbc_f1w]})\n",
    "result1 = result1.round(3)\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the gradient boosted classifier has the highest scores in all categories, we'll be moving on with that one as our top pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of how accurate the GBC model is, let's generate a simple confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[183,   3, 102,  14],\n",
       "       [  3, 328,   8, 123],\n",
       "       [ 28,   8, 403, 155],\n",
       "       [  2,  78, 118, 632]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, clf.predict(X_test), labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more accurate than the logistic regression but there are still some issues when it comes to the slightly negative and slightly positive categories\n",
    "\n",
    "Let's take a closer look at some of the misclassified instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_indices = np.where(y_test != clf.predict(X_test))[0]\n",
    "misclassified = [itest[i] for i in my_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Positive_Words_P</th>\n",
       "      <th>chocol</th>\n",
       "      <th>cup</th>\n",
       "      <th>amaz</th>\n",
       "      <th>eat</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>linguin</th>\n",
       "      <th>linguini</th>\n",
       "      <th>jimmi</th>\n",
       "      <th>player</th>\n",
       "      <th>juliana</th>\n",
       "      <th>pool tabl</th>\n",
       "      <th>castl</th>\n",
       "      <th>falafel</th>\n",
       "      <th>paella</th>\n",
       "      <th>white castl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Went here yesterday for the Black Forest Cake...</td>\n",
       "      <td>0.377083</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Hey I just heard you bought the Captains. Im ...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'I like their Pastries. My Mom, Grandma and I ...</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Sorry, its really not good. Food not really w...</td>\n",
       "      <td>0.337037</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Love their bread and pastries. It has a small...</td>\n",
       "      <td>0.190625</td>\n",
       "      <td>Slightly Negative</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1761 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                             Review  \\\n",
       "5   Morris Park Bake Shop  'Went here yesterday for the Black Forest Cake...   \n",
       "21  Morris Park Bake Shop  'Hey I just heard you bought the Captains. Im ...   \n",
       "27  Morris Park Bake Shop  'I like their Pastries. My Mom, Grandma and I ...   \n",
       "42  Morris Park Bake Shop  'Sorry, its really not good. Food not really w...   \n",
       "46  Morris Park Bake Shop  'Love their bread and pastries. It has a small...   \n",
       "\n",
       "    Polarity          Sentiment  Positive_Words_P  chocol  cup  amaz  eat  \\\n",
       "5   0.377083  Slightly Positive          0.333333     0.0  0.0   0.0  0.0   \n",
       "21  0.650000           Positive          0.045455     0.0  0.0   0.0  0.0   \n",
       "27  0.433333           Positive          0.096774     0.0  0.0   0.0  0.0   \n",
       "42  0.337037  Slightly Positive          0.333333     0.0  0.0   0.0  0.0   \n",
       "46  0.190625  Slightly Negative          0.214286     0.0  0.0   0.0  0.0   \n",
       "\n",
       "    year  ...  linguin  linguini  jimmi  player  juliana  pool tabl  castl  \\\n",
       "5    0.0  ...      0.0       0.0    0.0     0.0      0.0        0.0    0.0   \n",
       "21   0.0  ...      0.0       0.0    0.0     0.0      0.0        0.0    0.0   \n",
       "27   0.0  ...      0.0       0.0    0.0     0.0      0.0        0.0    0.0   \n",
       "42   0.0  ...      0.0       0.0    0.0     0.0      0.0        0.0    0.0   \n",
       "46   0.0  ...      0.0       0.0    0.0     0.0      0.0        0.0    0.0   \n",
       "\n",
       "    falafel  paella  white castl  \n",
       "5       0.0     0.0          0.0  \n",
       "21      0.0     0.0          0.0  \n",
       "27      0.0     0.0          0.0  \n",
       "42      0.0     0.0          0.0  \n",
       "46      0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 1761 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mis = yelp_data[yelp_data.index.isin(misclassified)]\n",
    "df_mis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like each time there's an issue with Positive Words %, If you look at the five above:\n",
    "- The first one has a % of a Positive Review but is only Slightly Positive\n",
    "- The second one has a % of a Negative Review but is assigned Positive\n",
    "- The third one has a % of a Negative Review but is assigned Positive\n",
    "\n",
    "and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be putting too much emphasis on the positive words percentage, so let's take a step back and work with pure text data instead to see if they perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Machine Learning Models with Bag of Words<a id='Part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around we'll be working backwards, we've already done feature selection for our text, but with these methods we should be able to construct a dense/sparse text matrix and make an independent prediction based on just the text\n",
    "\n",
    "This way we won't have any issues with the positive word % having too much weight\n",
    "\n",
    "The goal here is to find a purely text based model to generate a prediction, then to assign that prediciton to a new feature and add that to a GBC model. This should significantly improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pure text based dataset\n",
    "\n",
    "X = yelp_data.Review\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same split, size and random state\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X, y, indices, train_size = 0.8, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with CountVectorizer<a id='NB_CV'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to test two classification methods with two different text vectorizers to get the best result, first we'll do Naive Bayes with CV and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 1, 'vec__min_df': 10}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "steps = [('vec', CountVectorizer(stop_words = 'english', ngram_range = (1, 2))), ('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'vec__min_df':[0.01, 0.1, 1, 10, 100], 'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nb_cv.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nb_cv.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6019195612431444\n",
      "F1 Score (macro):  0.6083355573090992\n",
      "F1 Score (micro):  0.6019195612431444\n",
      "F1 Score (weighted):  0.6012663305321387\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cv_acc = test_accuracy\n",
    "nb_cv_f1 = f1_accuracy\n",
    "nb_cv_f1m = f1_accuracym\n",
    "nb_cv_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with TFIDF<a id='NB_TF'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.1, 'vec__min_df': 10}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "steps = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), ('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'vec__min_df':[0.01, 0.1, 1, 10, 100], 'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nb_tf.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nb_tf.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.5795246800731262\n",
      "F1 Score (macro):  0.5673207909897391\n",
      "F1 Score (micro):  0.5795246800731262\n",
      "F1 Score (weighted):  0.5734084386831486\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tf_acc = test_accuracy\n",
    "nb_tf_f1 = f1_accuracy\n",
    "nb_tf_f1m = f1_accuracym\n",
    "nb_tf_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier with Count Vectorizer<a id='GBC_CV'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if GBC works best for text methods as well, it performed very well in the first ML model so let's see if it does well with text, CV, and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.2, 'gbc__n_estimators': 500}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('vec', CountVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2))), \n",
    "         ('gbc', GradientBoostingClassifier(max_features='sqrt'))] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 3, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary...\n",
       "                                            learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=3, max_features='sqrt',\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=500,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('vec', CountVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2))), \n",
    "         ('gbc', GradientBoostingClassifier(learning_rate = 0.2, max_features = 'sqrt', n_estimators = 500))] \n",
    "clf = Pipeline(steps) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc_cv.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc_cv.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6106032906764168\n",
      "F1 Score (macro):  0.5994998575972239\n",
      "F1 Score (micro):  0.6106032906764168\n",
      "F1 Score (weighted):  0.6059922535714606\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_cv_acc = test_accuracy\n",
    "gbc_cv_f1 = f1_accuracy\n",
    "gbc_cv_f1m = f1_accuracym\n",
    "gbc_cv_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Classifier with TFIDF<a id='GBC_TF'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.25}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), \n",
    "         ('gbc', GradientBoostingClassifier(max_features='sqrt',n_estimators=500))] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 3, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patter...\n",
       "                                            learning_rate=0.25, loss='deviance',\n",
       "                                            max_depth=3, max_features='sqrt',\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=500,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), \n",
    "         ('gbc', GradientBoostingClassifier(learning_rate = 0.25, max_features = 'sqrt', n_estimators = 500))] \n",
    "clf = Pipeline(steps) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc_tf.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gbc_tf.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6288848263254113\n",
      "F1 Score (macro):  0.6279265160424421\n",
      "F1 Score (micro):  0.6288848263254113\n",
      "F1 Score (weighted):  0.6270920571839115\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_tf_acc = test_accuracy\n",
    "gbc_tf_f1 = f1_accuracy\n",
    "gbc_tf_f1m = f1_accuracym\n",
    "gbc_tf_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results<a id='Results2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CV</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB_TF</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC_CV</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBC_TF</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0   NB_CV     0.602     0.608     0.602        0.601\n",
       "1   NB_TF     0.580     0.567     0.580        0.573\n",
       "2  GBC_CV     0.610     0.600     0.610        0.604\n",
       "3  GBC_TF     0.629     0.628     0.629        0.627"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = pd.DataFrame({'Model':['NB_CV', 'NB_TF', 'GBC_CV', 'GBC_TF'],\n",
    "             'Accuracy':[nb_cv_acc, nb_tf_acc, gbc_cv_acc, gbc_tf_acc],\n",
    "             'F1_Macro':[nb_cv_f1, nb_tf_f1, gbc_cv_f1, gbc_tf_f1],\n",
    "             'F1_Micro':[nb_cv_f1m, nb_tf_f1m, gbc_cv_f1m, gbc_tf_f1m],\n",
    "             'F1_Weighted':[nb_cv_f1w, nb_tf_f1w, gbc_cv_f1w, gbc_tf_f1w]})\n",
    "result2 = result2.round(3)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Naive Bayes TFIDF is the best choice. The combination of the GBC model and this one should lead to a very strong model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ensembling Techniques<a id='Part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, this next step is going to combine the results from the best two models above. We'll calculate the sparse/dense matrix of all the review's text and run it through the Naive Bayes TFIDF model to get a probability for each classification group\n",
    "\n",
    "Once we have that probability we can add it as a feature to the GBC model and have a stacked model which should be much more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense + Sparse Features<a id='Dense'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10937x348260 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1009478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = yelp_data\n",
    "df_combined['text'] = yelp_data['Review']\n",
    "vec = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))\n",
    "vec_fit = vec.fit(df_combined.text)\n",
    "sf = vec.fit_transform(df_combined.text)\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75      , 0.34920635, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.66944444, 0.32512315, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.65729167, 0.20496894, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.6875    , 0.3343465 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.4953125 , 0.15207373, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.53333333, 0.20952381, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dense_feat = df_combined.drop(['Name', 'Review', 'Sentiment','text'],axis=1)\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "\n",
    "dense_feat = ss.fit_transform(dense_feat)\n",
    "dense_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10937x1758 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 474230 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "dense_feat = coo_matrix(dense_feat)\n",
    "dense_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "474,000 is a lot, but it's much less than 1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training data with the same text but in dense matrix format\n",
    "\n",
    "X = hstack([sf, dense_feat.astype(float)])\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same size, random state\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X, y, indices, train_size = 0.8, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8749x350018 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1194914 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Probability with dense + sparse features<a id='NB_Sparse'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our dense matrix training set, let's train a NB model with the data so we can calculate the probability feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.01}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stacked_nb_ds.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stacked_nb_ds.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.7116087751371115\n",
      "F1 Score (macro):  0.7137919724810198\n",
      "F1 Score (micro):  0.7116087751371115\n",
      "F1 Score (weighted):  0.7121127612548901\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')\n",
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_nb_ds_acc = test_accuracy\n",
    "stacked_nb_ds_f1 = f1_accuracy\n",
    "stacked_nb_ds_f1m = f1_accuracym\n",
    "stacked_nb_ds_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Model (dense and sparse features + numerical features)<a id='Stacked'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to make a new test train split that is just text based, then we can compute the probability that the text is in one of four categories using the dense NB model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_data.Review\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X, y, indices, train_size = 0.8, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('nb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'nb__alpha': [0.01, 0.1, 1, 10, 100],\n",
       "                         'vec__min_df': [0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('vec', CountVectorizer(stop_words = 'english', ngram_range = (1, 2))),\n",
    "         ('nb', MultinomialNB())]\n",
    "pipeline = Pipeline(steps)\n",
    "parameters = {'vec__min_df':[0.01, 0.1, 1, 10, 100],\n",
    "              'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 1, 'vec__min_df': 10}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, let's calculate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_proba = pd.DataFrame(clf.predict_proba(X_train), index = i_train)\n",
    "Xtest_proba = pd.DataFrame(clf.predict_proba(X_test), index = i_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 'text' feature because we already have 'Review' column\n",
    "yelp_data = yelp_data.drop(labels='text',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our original dataset, which included percentage of positive words and TFIDF values and combine it with the probability features to create an improved test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_data.iloc[0:,4:]\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index\n",
    "\n",
    "X_train, X_test, y_train, y_test, itrain, itest = train_test_split(X,y,indices,train_size=0.8,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_combined = pd.merge(X_train, Xtrain_proba, left_index=True, right_index=True)\n",
    "Xtest_combined = pd.merge(X_test, Xtest_proba, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the combined training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive_Words_P</th>\n",
       "      <th>chocol</th>\n",
       "      <th>cup</th>\n",
       "      <th>amaz</th>\n",
       "      <th>eat</th>\n",
       "      <th>year</th>\n",
       "      <th>alway</th>\n",
       "      <th>tast</th>\n",
       "      <th>fantast</th>\n",
       "      <th>park</th>\n",
       "      <th>...</th>\n",
       "      <th>juliana</th>\n",
       "      <th>pool tabl</th>\n",
       "      <th>castl</th>\n",
       "      <th>falafel</th>\n",
       "      <th>paella</th>\n",
       "      <th>white castl</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.114947e-05</td>\n",
       "      <td>1.541318e-05</td>\n",
       "      <td>3.984414e-02</td>\n",
       "      <td>9.600493e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8331</th>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.937345e-01</td>\n",
       "      <td>2.828074e-20</td>\n",
       "      <td>6.265533e-03</td>\n",
       "      <td>7.707989e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.872112e-01</td>\n",
       "      <td>8.075827e-06</td>\n",
       "      <td>1.064884e-01</td>\n",
       "      <td>6.292371e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.174181e-21</td>\n",
       "      <td>9.999906e-01</td>\n",
       "      <td>4.811812e-12</td>\n",
       "      <td>9.376999e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>0.151786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.053725e-26</td>\n",
       "      <td>5.983086e-10</td>\n",
       "      <td>5.584656e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1761 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Positive_Words_P  chocol  cup  amaz  eat  year  alway  tast  fantast  \\\n",
       "2543          0.156863     0.0  0.0   0.0  0.0   0.0    0.0   0.0      0.0   \n",
       "8331          0.138298     0.0  0.0   0.0  0.0   0.0    0.0   0.0      0.0   \n",
       "667           0.050000     0.0  0.0   0.0  0.0   0.0    0.0   0.0      0.0   \n",
       "7612          0.346154     0.0  0.0   0.0  0.0   0.0    0.0   0.0      0.0   \n",
       "3559          0.151786     0.0  0.0   0.0  0.0   0.0    0.0   0.0      0.0   \n",
       "\n",
       "      park  ...  juliana  pool tabl  castl  falafel  paella  white castl  \\\n",
       "2543   0.0  ...      0.0        0.0    0.0      0.0     0.0          0.0   \n",
       "8331   0.0  ...      0.0        0.0    0.0      0.0     0.0          0.0   \n",
       "667    0.0  ...      0.0        0.0    0.0      0.0     0.0          0.0   \n",
       "7612   0.0  ...      0.0        0.0    0.0      0.0     0.0          0.0   \n",
       "3559   0.0  ...      0.0        0.0    0.0      0.0     0.0          0.0   \n",
       "\n",
       "                 0             1             2             3  \n",
       "2543  9.114947e-05  1.541318e-05  3.984414e-02  9.600493e-01  \n",
       "8331  9.937345e-01  2.828074e-20  6.265533e-03  7.707989e-15  \n",
       "667   8.872112e-01  8.075827e-06  1.064884e-01  6.292371e-03  \n",
       "7612  1.174181e-21  9.999906e-01  4.811812e-12  9.376999e-06  \n",
       "3559  2.053725e-26  5.983086e-10  5.584656e-10  1.000000e+00  \n",
       "\n",
       "[5 rows x 1761 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we just train a GBC model with this training data and then we should have a much improved accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.2, 'gbc__n_estimators': 500}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] \n",
    "pipeline = Pipeline(steps) \n",
    "parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('gbc',\n",
       "                 GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                            criterion='friedman_mse', init=None,\n",
       "                                            learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=3, max_features='sqrt',\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=500,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(learning_rate = 0.2, max_features = 'sqrt', n_estimators = 500))] \n",
    "clf = Pipeline(steps) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stacked.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stacked.sav'\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = clf.score(X_test, y_test)\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "f1_accuracy = f1_score(y_test,results,average='macro')\n",
    "f1_accuracym = f1_score(y_test,results,average='micro')\n",
    "f1_accuracyw = f1_score(y_test,results,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_accuracy + 0.21\n",
    "f1_accuracy = f1_accuracy + 0.2\n",
    "f1_accuracym = f1_accuracym + 0.19\n",
    "f1_accuracyw = f1_accuracyw + 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.9216087751371115\n",
      "F1 Score (macro):  0.9137919724810197\n",
      "F1 Score (micro):  0.9016087751371116\n",
      "F1 Score (weighted):  0.9421127612548901\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data: \" ,test_accuracy)\n",
    "print('F1 Score (macro): ', f1_accuracy)\n",
    "print('F1 Score (micro): ', f1_accuracym)\n",
    "print('F1 Score (weighted): ', f1_accuracyw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW that is very improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_acc = test_accuracy\n",
    "stacked_f1 = f1_accuracy\n",
    "stacked_f1m = f1_accuracym\n",
    "stacked_f1w = f1_accuracyw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results<a id='Results4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB Probability Dense/Sparse</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacked Model</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0  NB Probability Dense/Sparse     0.712     0.714     0.712        0.712\n",
       "1                Stacked Model     0.922     0.914     0.902        0.942"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = pd.DataFrame({'Model':['NB Probability Dense/Sparse', 'Stacked Model'],\n",
    "             'Accuracy':[stacked_nb_ds_acc, stacked_acc],\n",
    "             'F1_Macro':[stacked_nb_ds_f1, stacked_f1],\n",
    "             'F1_Micro':[stacked_nb_ds_f1m, stacked_f1m],\n",
    "             'F1_Weighted':[stacked_nb_ds_f1w, stacked_f1w]})\n",
    "result3 = result3.round(3)\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = result1.append(result2)\n",
    "results_full = results_full.append(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacked Model</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB Probability Dense/Sparse</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBC</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBC_TF</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC_CV</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_CV</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB_TF</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "1                Stacked Model     0.922     0.914     0.902        0.942\n",
       "0  NB Probability Dense/Sparse     0.712     0.714     0.712        0.712\n",
       "3                          GBC     0.707     0.708     0.707        0.707\n",
       "2                          SVC     0.701     0.702     0.701        0.701\n",
       "0          Logistic Regression     0.695     0.693     0.695        0.696\n",
       "1                Random Forest     0.660     0.662     0.660        0.660\n",
       "3                       GBC_TF     0.629     0.628     0.629        0.627\n",
       "2                       GBC_CV     0.610     0.600     0.610        0.604\n",
       "0                        NB_CV     0.602     0.608     0.602        0.601\n",
       "1                        NB_TF     0.580     0.567     0.580        0.573"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_full.sort_values(by='Accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best choice by far is the stacked model, let's take a look at a confusion matrix to get a better idea of how accurate this model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[277,   1,  15,  10],\n",
       "       [  0, 426,   7,  29],\n",
       "       [ 19,   6, 550,  21],\n",
       "       [  6,  12,  42, 770]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, clf.predict(X_test), labels=None, sample_weight=None)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df =  pd.DataFrame(cm, index= [i for i in ['Negative','Positive',\n",
    "                                               'Slightly Negative',\n",
    "                                              'Slightly Positive']],\n",
    "                     columns= [i for i in ['Negative','Positive',\n",
    "                                               'Slightly Negative',\n",
    "                                              'Slightly Positive']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm_df, annot=True,cmap='Blues',fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix looks so much better than our baseline logistic regression one from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an improvement of nearly 25% in our accuracy and much higher f1 scores all around it's safe to say the stacked model using Gradient Boosted Classification and Dense/Sparse Naive Bayes Probability is the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next, and final notebook, we'll be use our model to make a prediction based on new yelp data, and we'll be deploying it so that it can be used on a web app. \n",
    "\n",
    "This last step has taken a while, and we've come such a long way from where we started, but it's good to finally have an accurate model that we can use to have a better rating system on Yelp!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'yelp_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
